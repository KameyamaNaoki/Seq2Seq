# -*- coding: utf-8 -*-
"""hid のコピー

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18b9itm4md7VM0F5KHo86qiyV27F_7RyH

隠れ層の次元を変える
"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.model_selection import train_test_split
import random
from sklearn.utils import shuffle
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import pandas as pd

# 各バッチデータの予測値の分散を求める
def cal_variance(y_str, p_str):

  y_float = float(y_str)
  p_float = float(p_str)

  # 分散もどきの計算
  tmp = ((y_float - p_float)**2)
  return tmp

#偏差の計算
def cal_deviation(var, nc):
  if nc==15000:
    nc=14999
  dec = np.sqrt(var/(15000-nc))
  return dec


#生成した文字列が数になっているかチェック
def check_number(p_str):
    f = 0

    if len(p_str) < 2:
      if p_str == '' or p_str == '-':
        f = 1
      
    else:
      for check in range(len(p_str)-1):
        if p_str[check+1]=='-':
          f = 1
    return f

# 数字の文字をID化
char2id = {str(i) : i for i in range(10)}

# 空白(10)：系列の長さを揃えるようのパディング文字
# -(11)：マイナスの文字
# _(12)：系列生成開始を知らせる文字
char2id.update({" ":10, "-":11, "_":12})

# 空白込みの３桁の数字をランダムに生成
def generate_number():
    number = [random.choice(list("0123456789")) for _ in range(random.randint(1, 3))]
    return int("".join(number))

# Decoderのアウトプットのテンソルから要素が最大のインデックスを返す。つまり生成文字を意味する
def get_max_index(decoder_output):
  results = []
  for h in decoder_output:
    results.append(torch.argmax(h))
  return torch.tensor(results, device=device).view(BATCH_NUM, 1)
  
# 確認
print(generate_number())
# 753

# 系列の長さを揃えるために空白パディング
def add_padding(number, is_input=True):
    number = "{: <7}".format(number) if is_input else "{: <5s}".format(number)
    return number

# 確認
num = generate_number()
print("\"" + str(add_padding(num)) + "\"")
# "636    "
# 7

# データ準備
input_data = []
output_data = []

# データを５００００件準備する
while len(input_data) < 50000:
    x = generate_number()
    y = generate_number()
    z = x - y
    input_char = add_padding(str(x) + "-" + str(y))
    output_char = add_padding("_" + str(z), is_input=False)

    # データをIDにに変換
    input_data.append([char2id[c] for c in input_char])
    output_data.append([char2id[c] for c in output_char])

# 確認
print(input_data[987])
print(output_data[987])
# [1, 5, 11, 2, 6, 6, 10]　（←"15-266"）
# [12, 11, 2, 5, 1]　（←"_-251"）

# ７：３にデータをわける
train_x, test_x, train_y, test_y = train_test_split(input_data, output_data, train_size= 0.7)


# データをバッチ化するための関数
def train2batch(input_data, output_data, batch_size=100):
    input_batch = []
    output_batch = []
    input_shuffle, output_shuffle = shuffle(input_data, output_data)
    for i in range(0, len(input_data), batch_size):
      input_batch.append(input_shuffle[i:i+batch_size])
      output_batch.append(output_shuffle[i:i+batch_size])
    return input_batch, output_batch

import torch
import torch.nn as nn
import torch.optim as optim


embedding_dim = 200 # 文字の埋め込み次元数

vocab_size = len(char2id) # 扱う文字の数。今回は１３文字

# GPU使う用
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Encoderクラス
class Encoder(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(Encoder, self).__init__()
        self.hidden_dim = hidden_dim
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=char2id[" "])
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)

    def forward(self, sequence):
        embedding = self.word_embeddings(sequence)
        # Many to Oneなので、第２戻り値を使う
        _, state = self.lstm(embedding)
        # state = (h, c)
        return state

# Decoderクラス
class Decoder(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(Decoder, self).__init__()
        self.hidden_dim = hidden_dim
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=char2id[" "])
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        # LSTMの128次元の隠れ層を13次元に変換する全結合層
        self.hidden2linear = nn.Linear(hidden_dim, vocab_size)

    def forward(self, sequence, encoder_state):
        embedding = self.word_embeddings(sequence)
        # Many to Manyなので、第１戻り値を使う。
        # 第２戻り値は推論時に次の文字を生成するときに使います。
        output, state = self.lstm(embedding, encoder_state)
        output = self.hidden2linear(output)
        return output, state



# 損失関数
criterion = nn.CrossEntropyLoss()




BATCH_NUM = 100
EPOCH_NUM = 2


hidvar_loss = []
all_losses = []
hid_dic = []
hid_col = []
hid_nc = []

rate_bar = [0, 0.2, 0.4, 0.6, 0.8, 1.0]
epock_bar = [0, 20, 40, 60, 80, 100]
#隠れ層の次元を指定
hid_bar = [1, 5]

print("training ...")
c=0
#print(len(hid_bar))
#print(hid_bar[0])
#print(range(len(hid_bar)))
for hidvar in range(len(hid_bar)):
  
  hidden_dim = hid_bar[hidvar] # LSTMの隠れ層のサイズ エンコーダの最終的な出力のサイズ
  print(hidden_dim)
  # GPU使えるように。
  encoder = Encoder(vocab_size, embedding_dim, hidden_dim).to(device)
  decoder = Decoder(vocab_size, embedding_dim, hidden_dim).to(device)
  # 最適化
  encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)
  decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001) 
  for epoch in range(1, EPOCH_NUM+1):
    epoch_loss = 0 # epoch毎のloss
      

    # データをミニバッチに分ける
    input_batch, output_batch = train2batch(train_x, train_y, batch_size=BATCH_NUM)

    for i in range(len(input_batch)):

        # 勾配の初期化
        encoder_optimizer.zero_grad()
        decoder_optimizer.zero_grad()

        # データをテンソルに変換
        input_tensor = torch.tensor(input_batch[i], device=device)
        output_tensor = torch.tensor(output_batch[i], device=device)

        # Encoderの順伝搬
        encoder_state = encoder(input_tensor)

        #print(encoder_state)

        # Decoderで使うデータはoutput_tensorを１つずらしたものを使う
        # Decoderのインプットとするデータ
        source = output_tensor[:, :-1]

        # Decoderの教師データ
        # 生成開始を表す"_"を削っている
        target = output_tensor[:, 1:]

        loss = 0
        # 学習時はDecoderはこのように１回呼び出すだけでグルっと系列をループしているからこれでOK
        # sourceが４文字なので、以下でLSTMが4回再帰的な処理してる
        decoder_output, _ = decoder(source, encoder_state)
        # decoder_output.size() = (100,4,13)
        # 「13」は生成すべき対象の文字が13文字あるから。decoder_outputの3要素目は
        # [-14.6240,  -3.7612, -11.0775,  ...,  -5.7391, -15.2419,  -8.6547]
        # こんな感じの値が入っており、これの最大値に対応するインデックスを予測文字とみなす

        for j in range(decoder_output.size()[1]):
            # バッチ毎にまとめてloss計算
            # 生成する文字は4文字なので、4回ループ
            loss += criterion(decoder_output[:, j, :], target[:, j])

        epoch_loss += loss.item()

        # 誤差逆伝播
        loss.backward()

        # パラメータ更新
        # Encoder、Decoder両方学習
        encoder_optimizer.step()
        decoder_optimizer.step()

    # 損失を表示
    if epoch % 10 == 0:
      print("Epoch %d: %.2f" % (epoch, epoch_loss))
    all_losses.append(epoch_loss)
    #if epoch_loss < 1: break

        # 評価用データ
    test_input_batch, test_output_batch = train2batch(test_x, test_y)
    input_tensor = torch.tensor(test_input_batch, device=device)

    predicts = []
    for i in range(len(test_input_batch)):
      with torch.no_grad(): # 勾配計算させない
        encoder_state = encoder(input_tensor[i])

        # Decoderにはまず文字列生成開始を表す"_"をインプットにするので、"_"のtensorをバッチサイズ分作成
        start_char_batch = [[char2id["_"]] for _ in range(BATCH_NUM)]
        decoder_input_tensor = torch.tensor(start_char_batch, device=device)

        # 変数名変換
        decoder_hidden = encoder_state

        # バッチ毎の結果を結合するための入れ物を定義
        batch_tmp = torch.zeros(100,1, dtype=torch.long, device=device)
        # print(batch_tmp.size())
        # (100,1)

        for _ in range(5):
          decoder_output, decoder_hidden = decoder(decoder_input_tensor, decoder_hidden)
          # 予測文字を取得しつつ、そのまま次のdecoderのインプットとなる
          decoder_input_tensor = get_max_index(decoder_output.squeeze())
          # バッチ毎の結果を予測順に結合
          batch_tmp = torch.cat([batch_tmp, decoder_input_tensor], dim=1)

        # 最初のbatch_tmpの0要素が先頭に残ってしまっているのでスライスして削除
        predicts.append(batch_tmp[:,1:])

    # バッチ毎の予測結果がまとまって格納されてます。
    #print(len(predicts))
    # 150
    #print(predicts[0].size())
    # (100, 5)
    #print(predicts)


    id2char = {str(i) : str(i) for i in range(10)}
    id2char.update({"10":"", "11":"-", "12":""})
    row = []
    #二乗誤差を保存する変数
    var_bar = 0
    #数になってない文字列が生成された数を保存する変数
    no_count = 0
    for i in range(len(test_input_batch)):
      batch_input = test_input_batch[i]
      batch_output = test_output_batch[i]
      batch_predict = predicts[i]

      
      for inp, output, predict in zip(batch_input, batch_output, batch_predict):
        x = [id2char[str(idx)] for idx in inp]
        y = [id2char[str(idx)] for idx in output]
        p = [id2char[str(idx.item())] for idx in predict]

        x_str = "".join(x)
        y_str = "".join(y)
        p_str = "".join(p)

        if (check_number(p_str)!=1):
          var_bar += cal_variance(y_str, p_str)
        else:
          no_count += 1


        
        #print(var_bar)

        judge = "O" if y_str == p_str else "X" #正誤をまるばつ
        row.append([x_str, y_str, p_str, judge])
    predict_df = pd.DataFrame(row, columns=["input", "answer", "predict", "judge"])
    #正答率を記録
    hid_col.append(len(predict_df.query('judge == "O"')) / len(predict_df))
    #偏差を記録
    hid_dic.append(cal_deviation(var_bar, no_count))
    #数になってない文字列の数を記録
    hid_nc.append(no_count)
    #変数をリセット
    var_bar = 0
    no_count = 0
    

  

  # 正解率を表示
  #print(len(predict_df.query('judge == "O"')) / len(predict_df))
  # 0.8492
  # 間違えたデータを一部見てみる
  #print(predict_df.query('judge == "X"').head(10))

#リストを二次元配列に変形 
hid_col = np.array(hid_col).reshape([(len(hid_bar)), EPOCH_NUM])
hid_dic = np.array(hid_dic).reshape([(len(hid_bar)), EPOCH_NUM])
hid_nc = np.array(hid_nc).reshape([(len(hid_bar)), EPOCH_NUM])

print("Done")
import matplotlib.pyplot as plt
# %matplotlib inline

#３種類のデータからそれぞれ最終エポックの回のみ抽出し、グラフ化
print(hid_col)
hid_col2 = hid_col[:,EPOCH_NUM-1]
fig = plt.figure()
ax = fig.add_subplot(111, title='COLLECT RATE' , xlabel='HIDDEN LAYER', ylabel='COLLECT RATE', ylim=(0, 1.0))
plt.plot(hid_col2, color = 'red')

print(hid_col2)

hid_dic2 = hid_dic[:,EPOCH_NUM-1]
fig = plt.figure()
ax = ax = fig.add_subplot(111, title='DEVIATION' , xlabel='HIDDEN LAYER', ylabel='DEVIATION')
plt.plot(hid_dic2, color = 'green')

print(hid_dic2)

print(hid_nc)
hid_nc2 = hid_nc[:,EPOCH_NUM-1]
fig = plt.figure()
ax = fig.add_subplot(111, title='NOT NUMBER' , xlabel='HIDDEN LAYER', ylabel='AMOUNT')
plt.plot(hid_nc2, color = 'blue')
print(hid_nc2)

# training ...
# Epoch 1: 1889.10
# Epoch 2: 1395.36
# Epoch 3: 1194.29
# Epoch 4: 1049.05
# Epoch 5: 931.19
# Epoch 6: 822.30
# 〜略〜
# Epoch 96: 4.47
# Epoch 97: 126.06
# Epoch 98: 32.81
# Epoch 99: 12.69
# Epoch 100: 6.20
# Done

#３種類のデータを3Dグラフ化
x = np.arange(1, EPOCH_NUM+1, 1)
y = np.array(hid_bar)

X, Y = np.meshgrid(x, y)

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d', title='CORRECT RATE' , xlabel='EPOCK', ylabel='HIDDEN LAYER', zlabel='CORRECT RATE', xticks = epock_bar, yticks = hid_bar, zticks = rate_bar, zlim=(0, 1.0))
ax.plot_wireframe(X, Y, hid_col, color = 'red')
plt.show()

print(hid_col)

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d', title='DEVIATION' , xlabel='EPOCK', ylabel='HIDDEN LAYER', zlabel='DEViATION', xticks = epock_bar, yticks = hid_bar)
ax.plot_wireframe(X, Y, hid_dic, color = 'green')
plt.show()

print(hid_dic)

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d', title='NOT NUMBER' , xlabel='EPOCK', ylabel='HIDDEN LAYER', zlabel='AMOUNT', xticks = epock_bar, yticks = hid_bar)
ax.plot_wireframe(X, Y, hid_nc, color = 'blue')
plt.show()

print(hid_nc)